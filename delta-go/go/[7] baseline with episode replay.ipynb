{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679ebaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange\n",
    "\n",
    "from typing import Any, Dict, Tuple, Optional\n",
    "from game_mechanics import GoEnv, choose_move_randomly, load_pkl, play_go, save_pkl\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330aa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_move(observation, legal_moves, neural_network: nn.Module) -> int:\n",
    "    \"\"\"Called during competitive play. It acts greedily given current state of the board and value\n",
    "    function dictionary. It returns a single move to play.\n",
    "\n",
    "    Args:\n",
    "        state:\n",
    "        \n",
    "    Returns:\n",
    "        move sampled from the policy network\n",
    "    \"\"\"\n",
    "    probs, value = my_network(observation, legal_moves)\n",
    "    probs = probs[0].cpu().detach().numpy()\n",
    "    move = np.random.choice(range(82), p=probs)\n",
    "    return move\n",
    "\n",
    "\n",
    "def choose_move_human(observation: np.ndarray, legal_moves:np.ndarray, neural_network: nn.Module) -> int:\n",
    "    print(observation)\n",
    "    i, j = [int(_) for _ in input().split(\" \")]\n",
    "\n",
    "    return (i-1)*9 + j-1\n",
    "\n",
    "def random_move(observation, legal_moves):\n",
    "    return random.choice(legal_moves)\n",
    "\n",
    "def choose_move_no_network_human(observation: np.ndarray, legal_moves: np.ndarray) -> int:\n",
    "    \"\"\"The arguments in play_game() require functions that only take the state as input.\n",
    "\n",
    "    This converts choose_move() to that format.\n",
    "    \"\"\"\n",
    "    return choose_move_human(observation, legal_moves, my_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9a2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class alpha_go_zero_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Linear(81,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.tower1 = nn.Sequential(\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,82)\n",
    "        )\n",
    "        \n",
    "        self.tower2 = nn.Sequential(\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, legal_moves):\n",
    "        illegal = lambda legal: [move not in legal for move in range(82)]\n",
    "        mask = torch.stack([torch.as_tensor(illegal(lm)) for lm in legal_moves])        \n",
    "\n",
    "        \n",
    "        x = rearrange(x, 'b w h -> b (w h)')\n",
    "        x = self.stem(x)\n",
    "        x1 = self.tower1(x)\n",
    "        x1 = x1.masked_fill(mask, -torch.inf)\n",
    "        x1 = F.softmax(x1, dim=-1)\n",
    "        x2 = self.tower2(x)\n",
    "        x2 = torch.tanh(x2)     \n",
    "            \n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd42bca",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2797179358.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [4]\u001b[1;36m\u001b[0m\n\u001b[1;33m    def stack(self, data):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Reservoir:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        \n",
    "    def append(self, observation, old_value, reward, done, legal_moves, chosen_move):\n",
    "        self.data.append((observation, old_value, reward, done, legal_moves, chosen_move))\n",
    "    \n",
    "    def sample_pop(self, size):\n",
    "        if size > len(self.data):\n",
    "            size = len(self.data)\n",
    "        random.shuffle(self.data)\n",
    "        sample = self.data[:size]\n",
    "        self.data = self.data[size:]\n",
    "        return self.stack(sample)\n",
    "    \n",
    "    def sample(self, size):\n",
    "        \n",
    "    \n",
    "    def stack(self, data):\n",
    "        observations = torch.stack([d[0] for d in data])\n",
    "        old_values = torch.as_tensor([d[1] for d in data], dtype=torch.float32)\n",
    "        rewards = torch.as_tensor([d[2] for d in data], dtype=torch.float32)\n",
    "        dones = torch.as_tensor([d[3] for d in data], dtype=torch.float32)\n",
    "        legal_moves = [d[4] for d in data]\n",
    "        chosen_moves = [d[5] for d in data]\n",
    "        return observations, old_values, rewards, dones, legal_moves, chosen_moves\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
