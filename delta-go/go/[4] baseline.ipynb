{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "3c1fde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange\n",
    "\n",
    "from typing import Any, Dict, Tuple, Optional\n",
    "from game_mechanics import GoEnv, choose_move_randomly, load_pkl, play_go, save_pkl\n",
    "from tqdm import notebook\n",
    "\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c61983f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_move(observation: np.ndarray, legal_moves: np.ndarray, neural_network: nn.Module) -> int:\n",
    "    observation = normalize(observation)\n",
    "    with torch.no_grad():\n",
    "        probs, value = my_network(observation, legal_moves)\n",
    "    probs = probs.cpu().detach().numpy()\n",
    "    move = np.random.choice(range(82), p=probs)\n",
    "    return move\n",
    "\n",
    "\n",
    "def choose_move_human(observation: np.ndarray, legal_moves:np.ndarray, neural_network: nn.Module) -> int:\n",
    "    i, j = [int(_) for _ in input().split(\" \")]\n",
    "\n",
    "    return (i-1)*9 + j-1\n",
    "\n",
    "def random_move(observation, legal_moves):\n",
    "    return random.choice(legal_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "7e3a46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class alpha_go_zero(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # This represents the shared layer(s) before the different heads        \n",
    "        self.layer1 = nn.Linear(81, 600)\n",
    "        self.layer2 = nn.Linear(600, 600)\n",
    "        \n",
    "        self.head1 = nn.Linear(600, 82)\n",
    "        self.head2 = nn.Linear(600, 1)   \n",
    "\n",
    "\n",
    "    def forward(self, x, legal_moves):\n",
    "        illegal_moves = [i for i in range(81) if i not in legal_moves]\n",
    "        # Run the shared layer(s)\n",
    "        x1 = rearrange(x, 'w h -> (w h)') #self.flatten(x)\n",
    "        x1 = self.layer1(x1)\n",
    "        x1 = F.elu(x1)\n",
    "        \n",
    "        x2 = self.layer2(x1)\n",
    "        x2 = F.elu(x2)\n",
    "        \n",
    "        x3 = self.layer2(x2)\n",
    "        x3 = F.elu(x3)\n",
    "        \n",
    "        \n",
    "        # Run the different heads with the output of the shared layers as input\n",
    "        # stochastic predictions\n",
    "        x4a = self.head1(x3)\n",
    "        x4a[illegal_moves] = -torch.inf\n",
    "        x4a = F.softmax(x4a, dim=-1)\n",
    "        \n",
    "        #value function\n",
    "        x4b = self.head2(x3)\n",
    "        x4b = torch.tanh(x4b)\n",
    "        \n",
    "        return x4a, x4b\n",
    "\n",
    "#         x = rearrange(x, 'w h -> (w h)') #self.flatten(x)\n",
    "#         x1 = self.head1(x)\n",
    "#         x1[illegal_moves] = -torch.inf\n",
    "#         x1 = F.softmax(x1, dim=-1)\n",
    "        \n",
    "#         x2 = self.head2(x)\n",
    "#         x2 = torch.tanh(x2)        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "#         return x1, x2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26e19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(observation: np.ndarray) -> torch.Tensor:\n",
    "    return torch.as_tensor(observation, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d45f3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(network, env):\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    observation, reward, done, info = env.reset()\n",
    "    while not done:\n",
    "        legal_moves = info['legal_moves']\n",
    "        observation = normalize(observation)\n",
    "        network_move = choose_move(observation, legal_moves, network)\n",
    "        observation, reward, done, info = env.step(network_move)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56099898",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "c45b588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR, ExponentialLR, ChainedScheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ca5a5a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "41c98316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f6272d575d445287b84d7fc7c81c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in notebook.tqdm(range(20)):\n",
    "    burn_in.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "515665be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(my_network.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "cfbc7710",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304fe390c73b4e19b1357e21af4c1659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e6e6bbdd2a4f55aacc7bf0912216b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [282]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     38\u001b[0m     legal_moves \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegal_moves\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 39\u001b[0m     probs, value \u001b[38;5;241m=\u001b[39m \u001b[43mmy_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegal_moves\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlegal_moves\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     chosen_move \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m82\u001b[39m), p\u001b[38;5;241m=\u001b[39mprobs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     41\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(chosen_move)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [277]\u001b[0m, in \u001b[0;36malpha_go_zero.forward\u001b[1;34m(self, x, legal_moves)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Run the different heads with the output of the shared layers as input\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# stochastic predictions\u001b[39;00m\n\u001b[0;32m     29\u001b[0m x4a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead1(x3)\n\u001b[1;32m---> 30\u001b[0m x4a[illegal_moves] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m     31\u001b[0m x4a \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(x4a, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#value function\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set up network & env:\n",
    "experiment_name = 'Baseline_L'\n",
    "my_network = alpha_go_zero()\n",
    "opponent_choose_move = random_move\n",
    "game_speed_multiplier=1000000\n",
    "render=False\n",
    "verbose=False\n",
    "env = GoEnv(\n",
    "    opponent_choose_move,\n",
    "    verbose=verbose,\n",
    "    render=render,\n",
    "    game_speed_multiplier=game_speed_multiplier,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(my_network.parameters(), lr=0.001)\n",
    "burn_in = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=10_000)\n",
    "\n",
    "metrics = []\n",
    "test_eval_size = []\n",
    "\n",
    "num_episodes = 10_000\n",
    "num_test_episodes = 25\n",
    "block_train_episodes = 100\n",
    "gamma = 0.9\n",
    "total_score = 0\n",
    "total_played = 0\n",
    "train_rewards = []\n",
    "train_losses = {\n",
    "    'policy': [],\n",
    "    'value': []\n",
    "}\n",
    "for episode in notebook.tqdm(range(num_episodes)):\n",
    "    old_observation, reward, done, info = env.reset()\n",
    "    old_value = 0 #torch.tensor(0, dtype=torch.float32)\n",
    "    old_observation = normalize(old_observation)\n",
    "    observation = old_observation\n",
    "    while not done:\n",
    "        legal_moves = info['legal_moves']\n",
    "        probs, value = my_network(observation, legal_moves = legal_moves)\n",
    "        chosen_move = np.random.choice(range(0,82), p=probs.detach().numpy())\n",
    "        observation, reward, done, info = env.step(chosen_move)\n",
    "        observation = normalize(observation)\n",
    "        \n",
    "\n",
    "        # train value function\n",
    "        optimizer.zero_grad()\n",
    "        if not done:\n",
    "            loss_v = (old_value - reward - value*gamma)**2\n",
    "            loss_policy = -torch.log(probs[chosen_move])*(reward + value.detach()*gamma)\n",
    "        if done:\n",
    "            loss_v = (value - reward)**2\n",
    "            loss_policy = -torch.log(probs[chosen_move])*(torch.Tensor([reward]))\n",
    "#         if episode % 300 == 50:\n",
    "#             print(f'{episode}:   {value.detach().numpy()[0].round(3)},    {loss_v.detach().numpy()[0].round(3)}')\n",
    "        \n",
    "        loss = loss_v + loss_policy # + entropy regularization?\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_losses['policy'].append(loss_policy.detach().numpy()[0])\n",
    "        train_losses['value'].append(loss_v.detach().numpy()[0])\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        old_value = value.detach()\n",
    "        old_observation = observation\n",
    "        \n",
    "    train_rewards.append(reward)\n",
    "    burn_in.step\n",
    "        \n",
    "    if episode % block_train_episodes == 0:\n",
    "        opponent_choose_move = random_move\n",
    "        test_env = GoEnv(\n",
    "            opponent_choose_move,\n",
    "            verbose=verbose,\n",
    "            render=render,\n",
    "            game_speed_multiplier=game_speed_multiplier,\n",
    "        )\n",
    "        rewards = [play_episode(my_network, test_env) for _ in notebook.tqdm(range(num_test_episodes))]\n",
    "        test_wr = sum([r == 1 for r in rewards])/num_test_episodes\n",
    "        test_score = sum(rewards)/num_test_episodes\n",
    "        test_ties = sum([r == 0 for r in rewards])/num_test_episodes\n",
    "        \n",
    "        train_wr = sum([r == 1 for r in train_rewards])/block_train_episodes\n",
    "        train_score = sum(train_rewards)/block_train_episodes\n",
    "        train_ties = sum([r == 0 for r in train_rewards])/block_train_episodes\n",
    "        train_rewards = []\n",
    "        metrics.append({'test_win_rate': test_wr,\n",
    "                        'test_score': test_score,\n",
    "                        'test_ties': test_ties,\n",
    "                        'train_win_rate': train_wr,\n",
    "                        'train_score': train_score,\n",
    "                        'train_ties': train_ties,\n",
    "                        'episode': episode,\n",
    "                        'total_score': total_score,\n",
    "                        'total_played': total_played,\n",
    "                        'train_loss_policy': sum(train_losses['policy'])/len(train_losses['policy']),\n",
    "                        'train_loss_value': sum(train_losses['value'])/len(train_losses['value'])\n",
    "                       })\n",
    "#         if train_score > 0.2: # if 60+% winrate, increase difficulty\n",
    "#             opponent_choose_move = partial(choose_move, neural_network=my_network)\n",
    "#             env = GoEnv(\n",
    "#                 opponent_choose_move,\n",
    "#                 verbose=verbose,\n",
    "#                 render=render,\n",
    "#                 game_speed_multiplier=game_speed_multiplier,\n",
    "#             )\n",
    "\n",
    "        pd.DataFrame(metrics).to_csv(f'logs/{experiment_name}_{episode}.csv')\n",
    "        train_losses = {\n",
    "            'policy': [],\n",
    "            'value': []\n",
    "        }\n",
    "\n",
    "    total_score += reward\n",
    "    total_played += 1\n",
    "#     print(round(total_score/total_played, 2), total_score, total_played, loss_policy.detach())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
