{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31eaf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from random import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env import CartPoleEnv\n",
    "from utils import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431790cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_policy = 0.005\n",
    "lr_value = 0.01\n",
    "gamma = 1.0\n",
    "\n",
    "policy = nn.Sequential(\n",
    "    nn.Linear(6, 20),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(20,2),\n",
    "    nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "value = nn.Sequential(\n",
    "    nn.Linear(6, 20),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(20,1)\n",
    ")\n",
    "\n",
    "optimizer_policy = torch.optim.Adam(policy.parameters(), lr=lr_policy)\n",
    "optimizer_value  = torch.optim.Adam(value.parameters(), lr=lr_value)\n",
    "\n",
    "env = CartPoleEnv()\n",
    "erm = EpisodeReplayMemory(gamma=gamma, lamda=1)\n",
    "num_episodes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "losses_value = []\n",
    "losses_policy = []\n",
    "# TODO: Add to the below training loop to train your policy gradients algorithm!\n",
    "for ep_num in tqdm(range(num_episodes)):\n",
    "    num_steps = 0\n",
    "    state, reward, done, _ = env.reset()\n",
    "\n",
    "    states = []\n",
    "    while not done:\n",
    "        prev_state = state\n",
    "        with torch.no_grad():\n",
    "            probs = policy(torch.as_tensor(state, dtype=torch.float32))\n",
    "            chosen_move = np.random.choice(range(0,2), p=probs.detach().numpy())\n",
    "        state, reward, done, _ = env.step(chosen_move)\n",
    "        num_steps += 1\n",
    "        if done and len(erm) >= 499: reward = 30\n",
    "        erm.append({'state': [state], \n",
    "                    'prev_state': [prev_state], \n",
    "                    'reward': reward,\n",
    "                    'done': done,\n",
    "                    'chosen_move': chosen_move,\n",
    "                    'prob_left': probs[chosen_move]})\n",
    "#     if len(erm) >= batch_size:\n",
    "    episode_len = len(erm)\n",
    "    sample = erm.sample_with_remove(episode_len)\n",
    "    \n",
    "    # fit value function\n",
    "    optimizer_value.zero_grad()\n",
    "    baseline_estimates = value(sample['prev_state'])\n",
    "    loss_value = F.smooth_l1_loss(baseline_estimates, sample['discounted_rewards'])\n",
    "    loss_value.backward()\n",
    "    optimizer_value.step()\n",
    "    losses_value.append(loss_value.item())\n",
    "    \n",
    "    # fit policy function\n",
    "    optimizer_policy.zero_grad()\n",
    "    probs = policy(sample['prev_state'])\n",
    "    moves = sample['chosen_move']\n",
    "    normalized_rewards = sample['discounted_rewards'] - baseline_estimates\n",
    "    loss_policy = -(torch.log(probs[range(episode_len),moves.long()])*sample['discounted_rewards']).sum()/episode_len\n",
    "    loss_policy.backward()\n",
    "    losses_policy.append(loss_policy.item())\n",
    "    optimizer_policy.step()\n",
    "        \n",
    "#     print(\"Ep:\", ep_num, \"Steps:\", num_steps)\n",
    "    steps.append(num_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
